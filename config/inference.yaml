postprocessing: 
    type: alnum  # none, alnum
    bias: 1.0

trainer:
    n_gpus: 1
    total_batch_size: 32

dataset:
    language: en
    batch_size: 64
    threads: 8
    tokenizer: ufal/byt5-small-multilexnorm2021-en

model:
    pretrained_lm: ufal/byt5-small-multilexnorm2021-en
    n_beams: 16
